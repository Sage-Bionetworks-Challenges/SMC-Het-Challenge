# Run the simulation over the given parameter values
#
# INPUT:
#    K.u.values - values of K.u to iterate over for the simulation
#    K.n.values - values of K.n to iterate over for the simulation
#    e1.values - values of epsilon1 to iterate over for the simulation
#    e2.values - values of epsilon2 to iterate over for the simulation
# OUTPUT:
#    res - nested list of lists with each dimension corresponding to
#       one of the iterated parameters from the input
run.sim <- function(K.u.values=2:11,
K.n.values=0:6,
e1.values=c(0,0.033,0.066,0.1),
e2.values=c(0,0.066,0.133,0.2)){
res <- list()
res <- lapply(K.u.values,
function(k.u){
res2 <- lapply(K.n.values,
function(k.n){
res3 <- lapply(e1.values,
function(e1){
res4 <- lapply(e2.values,
function(e2){
#print.sim()
sim.res.tot <- sapply(1:n.iter,
function(x){
data <- create.data(,,,k.u,k.n,e1,e2)
evaluate(data)
}
)
return(apply(data.frame(sim.res.tot), 1, mean))
})
names(res4) <- e2.values
res4
})
names(res3) <- e1.values
res3
})
names(res2) <- K.n.values
res2
})
names(res) <- K.u.values
return(res)
}
#### DATA OUTPUT/ANALYSIS ###################################
#### parse.data #############################################
# Extract simulation data with the given parameter values
# fixed. If a parameter value is not specified then all values
# of that parameter are returned.
# INPUT:
#     res - results of the simulation
#     K.u.value - value for K.u
#     K.n.value - value for K.n
#     e1.value - value for epsilon1
#     e2.value - value for epsilon2
# OUTPUT:
filter.data <- function(res,
K.u.value=NA,
K.n.value=NA,
e1.value=NA,
e2.value=NA){
if(!is.na(K.u.value)){
res <- parse.help(res, K.u.value, "K.u")
}
if(!is.na(K.n.value)){
for(val1 in names(res)){
res[[val1]] <- parse.help(res[[val1]], K.n.value, "K.n")
}
}
if(!is.na(e1.value)){
for(val1 in names(res)){
res1 <- res[[val1]]
for(val2 in names(res1)){
res1[[val2]] <- parse.help(res1[[val2]], e1.value, "epsilon1")
}
res[[val1]] <- res1
}
}
if(!is.na(e2.value)){
for(val1 in names(res)){
res1 <- res[[val1]]
for(val2 in names(res1)){
res2 <- res1[[val2]]
for(val3 in names(res2)){
res2[[val3]]<- parse.help(res2[[val3]], e2.value, "epsilon2")
}
res1[[val2]]<- res2
}
res[[val1]] <- res1
}
}
return(res)
}
# Helper function for parsing data
# INPUT:
#    res - nested list of lists of simulation results,
#       may be lower dimension than original simulation results
#    value - value to look set for the given parameter
#    param - name of the parameter
# OUTPUT:
#    res - nested list of lists of simulation results including
#       only results where the given parameter has the given value
filter.help <- function(res, value, param){
value.str <- toString(value)
if(is.null(res[[value.str]])){
warning(paste("Value ", value.str, " is not a valid index for ", param,
"\nReturning all values of ", param, " instead"))
} else{
temp <- res[[value.str]]
res <- list()
res[[value.str]] <- temp
}
return(res)
}
#### parse.data ############################################
# Parse simulation results to extract the dat in a usable
# form for a scatterplot.
#
# INPUT:
#    res - simulation results
#    params.fixed.values - vector of length 4 of parameter values that
#       are kept fixed throughout the simulation. Any parameters
#       that are set to be NA will be iterated over (at most one
#       parameter can be NA at a time).
#       Ordering of parameters: K.u, K.n, epsilon1, epsilon2
#    metrics - Optional vector of names of metrics to include in
#       the plot. If not included then all metrics are considered
# OUTPUT:
#    param.data - data frame containing the simulation results
#        where all but on of the parameters are fixed
parse.data <- function(res, params.fixed.values, metrics=NA){
# set names for easier access
names(params.fixed.values) <- c("K.u", "K.n", "epsilon1", "epsilon2")
# index of the unfixed parameter
param.ind <- match(NA, params.fixed.values)
if(length(param.ind) > 1){
stop("Too many parameter values are unspecified. Enter more parameters and try again")
}
# name of the unfixed parameter
param.name <- names(params.fixed.values)[param.ind]
# get the results we care about
res <- filter.data(res,
params.fixed.values[1],
params.fixed.values[2],
params.fixed.values[3],
params.fixed.values[4])
# values for the unfixed parameter
param.values <- switch(toString(param.ind),
'1' = names(res),
'2' = names(res[[1]]),
'3' = names(res[[1]][[1]]),
'4' = names(res[[1]][[1]][[1]])
)
# names of the different metrics being considered
if(is.na(metrics)){
metrics <- names(res[[1]][[1]][[1]][[1]])
}
# data frame for the ploting data
# will be populated later
param.data <- data.frame(matrix(nrow=length(metrics)*length(param.values),
ncol=3))
for(i in 1:length(param.values)){
val <- param.values[[i]]
params.values <- params.fixed.values
params.values[[param.name]] <- val
param.data[((i-1)*length(metrics)+1):(i*length(metrics)),] <- cbind(rep(as.numeric(val), length(metrics)),
res[[params.values]][metrics],
metrics)
}
names(param.data) <- c(param.name, "value", "metric")
param.data[,'value'] <- as.numeric(param.data[,'value'])
return(param.data)
}
#### parse.data.diff ##########################################
# Parse simulation results to extract the data in a usable
# form. Returns the difference between the metric scores for
# consecutive values of the unfixed parameter
#
# INPUT:
#    res - simulation results
#    params.fixed.values - vector of length 4 of parameter values that
#       are kept fixed throughout the simulation. Any parameters
#       that are set to be NA will be iterated over (at most one
#       parameter can be NA at a time).
#       Ordering of parameters: K.u, K.n, epsilon1, epsilon2
#    metrics - Optional vector of names of metrics to include in
#       the plot. If not included then all metrics are considered
# OUTPUT:
#    param.data - data frame containing the difference in metric
#        scores for consecutive values of the given unfixed parameter
#        where all other parameters are fixed
parse.data.diff <- function(res, params.fixed.values, metrics=NA){
parsed.data <- parse.data(res, params.fixed.values, metrics)
if(dim(parsed.data)[[1]] < 2){
stop("Too few data values to perform differencing. Try using parse.data instead")
}
# number of metrics
if(is.na(metrics)){
metrics <- unique(parsed.data[,3])
}
n.metrics <- length(metrics)
print(metrics)
print(n.metrics)
param.data <- data.frame(
matrix(nrow=(dim(parsed.data)[[1]]-n.metrics),
ncol=dim(parsed.data)[[2]])
)
names(param.data) <- c(paste(names(parsed.data)[[1]], ".diff", sep=""), "value", "metric")
param.data[,3] <- parsed.data[1:n.metrics,3]
print(param.data)
# a bit convoluted but relying on the structure of the parsed data
# essentially we are subtracting consecutive "blocks" of n.metrics rows
# from each other to get the differences, and then figuring out the names
for(i in 1:((dim(param.data)[[1]] / n.metrics))){
# factor level, the two parameter values that are differenced
lvl <- paste(parsed.data[(i*n.metrics),1],
"-",
parsed.data[((i+1)*n.metrics),1])
# differenced values
print(parsed.data[((i-1)*n.metrics+1):(i*n.metrics),2])
values <- parsed.data[((i-1)*n.metrics+1):(i*n.metrics),2] - parsed.data[(i*n.metrics+1):((i+1)*n.metrics),2]
param.data[((i-1)*n.metrics+1):(i*n.metrics),-3] <- cbind(lvl, values)
}
param.data[,2] <- as.numeric(param.data[,2])
return(param.data)
}
#### plot.sim ###############################################
# Plot the changes in one or more scoring metrics as a given
# parameter changes
#
# INPUT:
#    res - simulation results
#    params.fixed.values - vector of length 4 of parameter values that
#       are kept fixed throughout the simulation. Any parameters
#       that are set to be NA will be iterated over (at most one
#       parameter can be NA at a time).
#       Ordering of parameters: K.u, K.n, epsilon1, epsilon2
#    metrics - Optional vector of names of metrics to include in
#       the plot. If not included then all metrics are considered
#    diff - if true then plot the difference of the metric scores
#       evaluated for consecutive values of the given parameter,
#       otherwise plot the metric scores themselves
# OUTPUT:
#    plot - plot of the given data
plot.sim <- function(res, params.fixed.values, metrics=NA, diff=FALSE){
# get the data for plotting
if(diff){
plot.data <- parse.data.diff(res, params.fixed.values, metrics)
# need to use factors as independent variable instead of the actual
# metric values if using the difference
param.formula <- paste("factor(", names(plot.data)[[1]],")", sep="")
ylab <- "Metric Score Difference"
title <- paste("Effect of",
names(plot.data)[[1]],
"on Changes in Scoring Metric Value")
} else{
plot.data <- parse.data(res, params.fixed.values, metrics)
param.formula <- paste("factor(", names(plot.data)[[1]],")", sep="")
ylab <- "Metric Score"
title <- paste("Effect of",
names(plot.data)[[1]],
"on Scoring Metric")
}
print("Assigned data...")
# name of the parameter that is not being held fixed and
# the metrics that are being considered
data.names <- names(plot.data)
# formula for scatter plot
plot.formula <- as.formula(paste("value ~", param.formula))
print("Calculated formula...")
# details of simulation for plot
metrics <- unique(plot.data[,3])
levels <- unique(plot.data[,1])
descrip <- paste("Metrics:",
metrics,
" Parameters",
get.param.str(params.fixed.values),
collapse=",")
print("Created description...")
# calculate limits for y-axis
ymin <- min(plot.data[,2])
ymax <- max(plot.data[,2])
yrange <- ymax - ymin
if(yrange != 0){
ylim <- c(max(ymin - yrange/4, -0.1), min(ymax + yrange/4, 1.1))
} else {
ylim = c(-0.1, 1.1)
}
print("Calculated y limit...")
# assign graphical values
p.pch <- 19
p.colours <- default.colours(length(metrics))
p.cex = 1
# create legend
metric.legend <- list(
legend = list(
pch = p.pch,
cex = p.cex,
colours = p.colours,
labels = metrics,
title = 'Metrics',
border = 'transparent'
)
);
metric.legend.grob <- legend.grob(
legends = metric.legend
);
create.scatterplot(plot.formula,
plot.data,
group = plot.data$metric,
pch = p.pch,
col = p.colours,
cex = p.cex,
legend = list(right = list(fun = metric.legend.grob)),
abline.h = c(0,1),
abline.col = 'grey',
xlab.label = data.names[[1]],
ylab.label = ylab,
xaxis.cex = 1,
yaxis.cex = 1,
xlab.cex = 1.5,
ylab.cex = 1.5,
main = title,
main.cex = 1.5,
description = descrip,
ylimits = ylim
)
}
# Takes in a data frame with multiple metrics in different columns
# and returns a data frame with all the data in 2 columns
make.2d.help <- function(plot.data){
# form the data into a data frame with only two columns
plot.data.2d <- data.frame(matrix(ncol=2))
names(plot.data.2d) <- c(names(plot.data)[[1]], "score")
for(i in 2:dim(plot.data)[[2]]){
temp <- plot.data[,c(1,i)]
names(temp) <- names(plot.data.2d)
plot.data.2d <- rbind(plot.data.2d, temp, make.row.names=F)
}
plot.data.2d <- plot.data.2d[-1,]
return(plot.data.2d)
}
#### EVALUATION METRICS #####################################
#### pseudo.v.metric ########################################
# Evaluates the clustering assingment based on the pseudo
# v-measure, using the K-L divergence between the true matrix
# (the classes) and the predicted matrix (the clusters).
# Can give the pseudo v-measure and/or the symmetrical pseudo
# v-measure.
#
# INPUT:
#   data - matrix of size (K.u+K.n x C) that gives the number of
#       elements of each class that are assigned to each cluster
#   epsilon - small, nonzero number to be used instead of zero
#       in the K-L divergence
#   version - vector of strings corresponding to which measures to
#       calculate
#       "normal" - regular pseudo v-measure
#       "sym" - symmetrical
# OTUPUT:
#   score - score for the clustering assignment, higher is worse
pseudo.v.metric <- function(data, epsilon, version=c("normal","sym")){
ccms <- get.ccm(data)
# add epsilon to everything, to ensure no zero values
if(output){
print("Replacing zeros...")
}
ccm.t <- ccms[[1]] + epsilon
ccm.p <- ccms[[2]] + epsilon
# row normalize
if(output){
print("Normalizing rows...")
}
ccm.t <- row.normalize.help(ccm.t)
ccm.p <- row.normalize.help(ccm.p)
if(output){
print("Calculating divergence...")
}
score <- list()
score$normal <- kl.divergence(ccm.t,ccm.p)
score$sym <- (score$normal + kl.divergence(ccm.t,ccm.p)) / 2
return(score)
}
#### pseudo.v.metric ########################################
# Evaluates the clustering assingment based on the symmetrical pseudo
# v-measure, using the avg of K-L divergence between the true matrix
# (the classes) and the predicted matrix (the clusters) and the K-L
# divergence between the predicted matrix and the true matrix
#
# INPUT:
#   data - matrix of size (K.u+K.n x C) that gives the number of
#       elements of each class that are assigned to each cluster
#   epsilon - small, nonzero number to be used instead of zero
#       in the K-L divergence
# OTUPUT:
#   score - score for the clustering assignment, higher is worse
pseudo.v.sym.metric <- function(data, epsilon){
ccms <- get.ccm(data)
# add epsilon to everything, to ensure no zero values
if(output){
print("Replacing zeros...")
}
ccm.t <- ccms[[1]] + epsilon
ccm.p <- ccms[[2]] + epsilon
# row normalize
if(output){
print("Normalizing rows...")
}
ccm.t <- row.normalize.help(ccm.t)
ccm.p <- row.normalize.help(ccm.p)
if(output){
print("Calculating divergence...")
}
return(kl.divergence(ccm.t,ccm.p))
}
# Row normalize a matrix so all the row sums are 1
#
# INPUT:
#     mat - matrix to be normalized
# OUTPUT:
#     mat.new - normalized matrix
row.normalize.help <- function(mat){
t(apply(mat, 1, function(row){row / sum(row)}))
}
#### kl.divergence ########################################
# Calculate the K-L divergence between two matrices
#
# INPUT:
#     p - first matrix (reference)
#     q - second matrix (new)
# OUTPUT:
#     kl - K-L divergence
kl.divergence <- function(p,q){
sum(p * (log(p) - log(q)))
}
#### combinatorial.metric ###################################
# Evaluates the clustering assignment based on a weighted sum of
# the True Positive Rate and the True Negative Rate for
# co-clustering of elements. Returns a normalized score between
# 0 and 1.
#
# INPUT:
#    t - weight for True Positive Rate
#    f - weight for True Negative Rate
#    data - matrix of size (K.u+K.n x C) that gives the number of
#       elements of each class that are assigned to each cluster
# OUTPUT:
#    score - score for the clustering assignment, 0 = worst score,
#       1 = best score
combinatorial.metric <- function(t, f, data){
return((t * tpr(data) + f * tnr(data)) / (t + f))
}
#### tpr  ###############################################
# Returns the True Positive Rate for the given clustering
# assignment
#
# INPUT:
# data - matrix of size (K.u+K.n x C) that gives the number of
#     elements of each class that are assigned to each cluster
# OUTPUT:
# tpr - True Positive Rate
tpr <- function(data){
# NOTE: notation: lower case => local variable
c <- dim(data)[[2]]
n.c <- sum(data[,1])
# number of pairs of elements that are correctly assigned
# to the same cluster
tp <- sum(apply(data, c(1,2), function(x){x*(x-1)})) / 2
# number of pairs of elements that should be in the
# same cluster
tp.total <- (n.c*(n.c-1)*c) / 2
tpr <- tp / tp.total
return(tpr)
}
#### tnr  ###############################################
# Returns the True Negative Rate for the given clustering
# assignment
#
# INPUT:
# data - matrix of size (K.u+K.n x C) that gives the number of
#     elements of each class that are assigned to each cluster
# OUTPUT:
# tnr - True Negative Rate
tnr <- function(data){
# NOTE: notation: lower case => local variable
c <- dim(data)[[2]]
n.c <- sum(data[,1])
# number of pairs of elements that are incorrectly assigned
# to the same clusters
fn <- sum(apply(data, 1, tn.help)) / 2
# number of pairs of elements that should be assigned to
# different clusters
tn.total <- (n.c * (n.c * (c-1)) * c) / 2
tnr <- 1 - (fn / tn.total)
return(tnr)
}
# INPUT:
# row - vector of length C that gives the number of
#     elements of each class that are assigned to the given cluster
# OUTPUT:
# fn - False Negatives for the given cluster
fn.help <- function(row){
#number of elements in the given cluster
n.cluster <- sum(row)
# returns (elements both in the class and cluster) * [(total # elements)
#         - (# elements in the cluster) - (# elements in the class outside the cluster)]
fn <- sum(sapply(row,function(x){x * (n.cluster-x)}))
return(fn)
}
results <- run.sim()
results
results <- run.sim()
